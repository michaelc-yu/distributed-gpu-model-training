How to train large neural networks on multiple GPUs?

Training parallelism distributes the data or model parameters across multiple GPUs to lessen the enormous demand for GPU memory.
